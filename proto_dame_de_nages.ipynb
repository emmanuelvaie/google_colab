{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OvRt5HYFvjvs7rUUflrwgRPHjAP5gfaW",
      "authorship_tag": "ABX9TyNb1LJ94/t2H4kaYENSGYx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelvaie/google_colab/blob/main/proto_dame_de_nages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcTDp9NsHWx"
      },
      "source": [
        "#Get references data\n",
        "Find Mattieu's file and read it into a dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4rsDYrFr8GR"
      },
      "source": [
        "# piece of code that get a token\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# authenticate the user\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHlhhC2IwO6K"
      },
      "source": [
        "Once authenticated, list the files that contains personal records for the atheletes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmM_u1sywXSy",
        "outputId": "3725667a-310a-4755-e03a-21468410fd4d"
      },
      "source": [
        "# Auto-iterate through all files that matches this query\n",
        "#file_list = drive.ListFile({'q': \"'root' in parents and title contains 'Champs' and trashed=false\"}).GetList()\n",
        "#sharedWithMe=true \n",
        "# id = 1VJMXVNSxIv4li8yK7pXZ3sitWEdC8z-93z_Apkl77_Q\n",
        "file_list = drive.ListFile({'q': \"trashed=false and title contains '-Nico - ' and title contains '2022'\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: 2022.10.08-Nico - 1.csv, id: 1Lo_LeTSpsU1XvKA26-sXJU03MsYklGIu\n",
            "title: 2022.05.21-Nico - 5.csv, id: 1sAdUcDz4vxaMGMsCgcWc2i2uC6jVA5dv\n",
            "title: 2022.05.15-Nico - 3.csv, id: 1a-OP8ilFEVL_bCioayaGJGDi1fh7M-Ol\n",
            "title: 2022.05.15-Nico - 2.csv, id: 12kZ98a6IHGCWbQfguBA0Cb56CL2-REHL\n",
            "title: 2022.05.15-Nico - 1.csv, id: 1E0nwCTgYgxOqU-tXJ3MludLZe4zF4f3Q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we have the list\n",
        "ddd_list = list()\n",
        "lines2skip = {}\n",
        "\n",
        "for ddd_file in file_list:\n",
        "  file_id = ddd_file['id']\n",
        "  # we are going to create an temp file in GDrive which is file.csv\n",
        "  # this assume pyDrive has a token to read the GoogleDrive\n",
        "  myfile = drive.CreateFile({'id': file_id})\n",
        "  myfile.GetContentFile('file.csv')\n",
        "  file1 = open('file.csv', 'r')\n",
        "  count = 0\n",
        "\n",
        "  for line in file1:\n",
        "    count += 1\n",
        "    #print(\"Line{}: {}\".format(count, line.strip()))\n",
        "    if line.startswith('Interval Summaries:') and count==19:\n",
        "      #print('Line to skip is 28')\n",
        "      lines2skip[file_id] = 28   \n",
        "      break  \n",
        "    else:\n",
        "      if count==19:\n",
        "        #print('Line to skip is 18')\n",
        "        lines2skip[file_id] = 18     \n",
        "        \n",
        "# Closing files\n",
        "file1.close()\n",
        "lines2skip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl1Cbh-_EAYl",
        "outputId": "a0243172-4b08-4e63-cf7d-f4bc573b5470"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1Lo_LeTSpsU1XvKA26-sXJU03MsYklGIu': 28,\n",
              " '1sAdUcDz4vxaMGMsCgcWc2i2uC6jVA5dv': 18,\n",
              " '1a-OP8ilFEVL_bCioayaGJGDi1fh7M-Ol': 18,\n",
              " '12kZ98a6IHGCWbQfguBA0Cb56CL2-REHL': 18,\n",
              " '1E0nwCTgYgxOqU-tXJ3MludLZe4zF4f3Q': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvSnNJI_BCJ"
      },
      "source": [
        "once the id of the spreadsheet retrieved, load it. \n",
        "But first, we need to be allowed Gspread  / GSheet to access to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHdv2xxc_KEd"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24H9eilMDx8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Jeqyi8eGCi"
      },
      "source": [
        "# Get the data from the outriggers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ddL5otne9Wq",
        "outputId": "1ce6abc7-313d-482f-ca43-32a9b86ac95e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# we have the list\n",
        "ddd_list = list()\n",
        "for ddd_file in file_list:\n",
        "  # collect meta data from the filename\n",
        "  filename = ddd_file['title']\n",
        "  file_id = ddd_file['id']\n",
        "  tokens = filename.split('-')\n",
        "  rower = tokens[1].strip().lower()\n",
        "  date = tokens[0].split('.')[2].strip() + '/' + tokens[0].split('.')[1].strip() + '/' + tokens[0].split('.')[0].strip()\n",
        "  training_type = 'tour-'+ tokens[2].strip() \n",
        "  print('rower: %s, training type: %s, date: %s' % (rower, training_type, date))\n",
        "  \n",
        "  # this assume pyDrive has a token to read the GoogleDrive\n",
        "  myfile = drive.CreateFile({'id': file_id})\n",
        "  myfile.GetContentFile('file.csv')\n",
        "  \n",
        "  # then read it in a Dataframe\n",
        "  skiprows = lines2skip[file_id]\n",
        "  ddd = pd.read_csv('file.csv', skiprows=skiprows, header=None)\n",
        "\n",
        "  # clean and organize the data\n",
        "  # column names un 1st row\n",
        "  # reformat\n",
        "  headers = ddd.iloc[0].apply(lambda x: str(x).replace(' ', '_'))\n",
        "  headers = headers.apply(lambda x: str(x).replace('(', ''))\n",
        "  headers = headers.apply(lambda x: str(x).replace(')', ''))\n",
        "  headers = headers.apply(lambda x: str(x).replace('.', ''))\n",
        "  headers = headers.apply(lambda x: str(x).lower())\n",
        "\n",
        "  # rename columns\n",
        "  ddd = ddd.rename(columns=headers)\n",
        "\n",
        "  # dropping NA values\n",
        "  ddd2 = ddd.dropna()\n",
        "\n",
        "  # drop the 2 first lines\n",
        "  ddd2 = ddd2.drop([0, 1])\n",
        "\n",
        "  # drop the 2 last rows\n",
        "  #ddd2 = ddd2.drop(ddd2.tail(2).index)\n",
        "\n",
        "  # reset the index\n",
        "  ddd2 = ddd2.reset_index(drop=True)\n",
        "\n",
        "  # changing column types\n",
        "  column_names = list(ddd2.columns)\n",
        "\n",
        "  # datetime columns\n",
        "  # remove lines with date time columns formated to '---'\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['elapsed_time'].map(lambda x: str(x) == '---')].index)\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['split_gps'].map(lambda x: str(x) == '---')].index)\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['split_imp'].map(lambda x: str(x) == '---')].index)\n",
        "\n",
        "  # iterating on columns and change the types to string\n",
        "  for col in column_names:\n",
        "      ddd2[col] = ddd2[col].apply(lambda x: str(x).replace('---', '0'))\n",
        "\n",
        "  ddd2 = ddd2.convert_dtypes()\n",
        "\n",
        "  # removing datetime columns from the columns list\n",
        "  column_names.pop(3)  # 'elapsed_time'\n",
        "  column_names.pop(3)  # 'split_gps'\n",
        "  column_names.pop(4)  # 'split_imp'\n",
        "\n",
        "  # add meta data\n",
        "  ddd2['rower'] = rower\n",
        "  ddd2['training_type'] = training_type\n",
        "  ddd2['date'] = date\n",
        "\n",
        "  for col in column_names:\n",
        "    ddd2[col] = pd.to_numeric(ddd2[col])\n",
        "\n",
        "  # add columns amplitude and amplitude_efficace\n",
        "  ddd2['amplitude'] = -1*(ddd2['catch'] - ddd2['finish'])\n",
        "  ddd2['amplitude_efficace'] = - ddd2['catch'] - ddd2['slip'] + ddd2['finish'] - ddd2['wash']\n",
        "\n",
        "  # add loc in google format\n",
        "  ddd2['loc'] = ddd2['gps_lat'].apply(lambda x: str(x)) + ',' + ddd2['gps_lon'].apply(lambda x: str(x))\n",
        "\n",
        " \n",
        "  # accumulate ddd\n",
        "  ddd_list.append(ddd2)\n",
        "\n",
        "# concat dataframes\n",
        "all_ddd = pd.concat(ddd_list)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rower: nico, training type: tour-1.csv, date: 08/10/2022\n",
            "rower: nico, training type: tour-5.csv, date: 21/05/2022\n",
            "rower: nico, training type: tour-3.csv, date: 15/05/2022\n",
            "rower: nico, training type: tour-2.csv, date: 15/05/2022\n",
            "rower: nico, training type: tour-1.csv, date: 15/05/2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ddd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GX4CxD8LBJI",
        "outputId": "d3d68f07-8314-45cb-c41d-ef1c98ada8ce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "interval               1563\n",
              "distance_gps           1563\n",
              "distance_imp           1563\n",
              "elapsed_time           1563\n",
              "split_gps              1563\n",
              "speed_gps              1563\n",
              "split_imp              1563\n",
              "speed_imp              1563\n",
              "stroke_rate            1563\n",
              "total_strokes          1563\n",
              "distance/stroke_gps    1563\n",
              "distance/stroke_imp    1563\n",
              "heart_rate             1563\n",
              "power                  1563\n",
              "catch                  1563\n",
              "slip                   1563\n",
              "finish                 1563\n",
              "wash                   1563\n",
              "force_avg              1563\n",
              "work                   1563\n",
              "force_max              1563\n",
              "max_force_angle        1563\n",
              "gps_lat                1563\n",
              "gps_lon                1563\n",
              "rower                  1563\n",
              "training_type          1563\n",
              "date                   1563\n",
              "amplitude              1563\n",
              "amplitude_efficace     1563\n",
              "loc                    1563\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ddd.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82R_YMsjKI0M",
        "outputId": "3c44f423-eb6f-4db1-fd1a-6db97eb2f9a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "interval                 int64\n",
              "distance_gps           float64\n",
              "distance_imp           float64\n",
              "elapsed_time            string\n",
              "split_gps               string\n",
              "speed_gps              float64\n",
              "split_imp               string\n",
              "speed_imp              float64\n",
              "stroke_rate            float64\n",
              "total_strokes            int64\n",
              "distance/stroke_gps    float64\n",
              "distance/stroke_imp    float64\n",
              "heart_rate               int64\n",
              "power                    int64\n",
              "catch                    int64\n",
              "slip                     int64\n",
              "finish                   int64\n",
              "wash                     int64\n",
              "force_avg                int64\n",
              "work                     int64\n",
              "force_max                int64\n",
              "max_force_angle          int64\n",
              "gps_lat                float64\n",
              "gps_lon                float64\n",
              "rower                   object\n",
              "training_type           object\n",
              "date                    object\n",
              "amplitude                int64\n",
              "amplitude_efficace       int64\n",
              "loc                     object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31JzST2GTg4"
      },
      "source": [
        "Adding a workseet with the dame de nage data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX8Q04tMH7yO"
      },
      "source": [
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "\n",
        "db_sh = gc.open_by_key('1u4KNFewcNQ7YpfTW0L1S2RM6Wu87xar-5zFzvXa2T2o')\n",
        "\n",
        "worksheet_data = db_sh.worksheet('database')\n",
        "gd.set_with_dataframe(worksheet_data,all_ddd, include_index=False,include_column_header=True,resize=True)"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}